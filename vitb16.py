# -*- coding: utf-8 -*-
"""ViTB16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18HF3WrTz4p__zVYTlGOqerw8PYK3L5ZU
"""

import kagglehub
import shutil
from PIL import Image
import numpy as np
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import glob
import os
from collections import defaultdict
from pathlib import Path
import random
import torch
from collections import Counter
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from skimage.segmentation import mark_boundaries
import re
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import time
import copy
import timm
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler

path = kagglehub.dataset_download("ninadaithal/imagesoasis")
print("Original path:", path)

target_path = "/content/novel_approach"
if not os.path.exists(target_path):
    shutil.copytree(path, target_path)
print(f"Dataset convert to: {target_path}")

image_count = defaultdict(int)
class_folders = []

for root, dirs, files in os.walk(target_path):
    for file in files:
        if file.lower().endswith((".jpg")):
            class_name = Path(root).name
            image_count[class_name] += 1
    if not dirs and files:
        class_folders.append(root)

print("\n Images per class:")
for cls, count in image_count.items():
    print(f" - {cls}: {count} images")

print("\n Image example from each class:")
for class_path in class_folders:
    class_name = Path(class_path).name
    sample_images = [f for f in os.listdir(class_path) if f.endswith((".jpg"))]
    if not sample_images:
        continue
    img_path = os.path.join(class_path, sample_images[0])
    img = Image.open(img_path)

    plt.figure()
    plt.imshow(img, cmap='gray')
    plt.title(f"{class_name} – {img.size} – mode: {img.mode}")
    plt.axis("off")
    plt.show()

image_paths = []
labels = []
class_to_label = {}

for idx, class_path in enumerate(sorted(class_folders)):
    class_name = Path(class_path).name
    class_to_label[class_name] = idx
    for fname in os.listdir(class_path):
        if fname.lower().endswith((".jpg")):
            image_paths.append(os.path.join(class_path, fname))
            labels.append(idx)

print("Total images:", len(image_paths))
print("Class-to-label mapping:", class_to_label)

train_paths, temp_paths, train_labels, temp_labels = train_test_split(
    image_paths, labels, test_size=0.30, stratify=labels, random_state=42
)
val_paths, test_paths, val_labels, test_labels = train_test_split(
    temp_paths, temp_labels, test_size=0.50, stratify=temp_labels, random_state=42
)

splits = {
    'train': (train_paths, train_labels),
    'val': (val_paths, val_labels),
    'test': (test_paths, test_labels),
}

def save_images(split_name, paths, labels):
    for path, label in tqdm(zip(paths, labels), desc=f"Lagrer {split_name}", total=len(paths)):
        img = Image.open(path).convert("RGB")
        img = img.resize((224, 224))

        class_name = list(class_to_label.keys())[label]
        folder_name = f"{label}_{class_name.replace(' ', '_')}"
        target_dir = os.path.join("/content/datasetsplit", split_name, folder_name)
        os.makedirs(target_dir, exist_ok=True)

        fname = os.path.basename(path)
        save_path = os.path.join(target_dir, fname)
        img.save(save_path)

for split_name, (paths, labels) in splits.items():
    save_images(split_name, paths, labels)

print("\n Datasetsplit done")

def add_salt_and_pepper(image, amount=0.02):
    np_img = np.array(image)
    h, w, c = np_img.shape
    num_pixels = int(amount * h * w)

    for _ in range(num_pixels):
        y = random.randint(0, h - 1)
        x = random.randint(0, w - 1)
        salt_or_pepper = random.choice([0, 255])
        np_img[y, x] = [salt_or_pepper] * c

    return Image.fromarray(np_img.astype(np.uint8))


augmentation_config = {
    "0_Mild_Dementia": {"num_augment": 5, "fraction": 1.0, "overwrite": False},
    "1_Moderate_Dementia": {"num_augment": 5, "fraction": 1.0, "overwrite": False},
    "2_Non_Demented": {"num_augment": 0, "fraction": 0.25, "overwrite": True},
    "3_Very_mild_Dementia": {"num_augment": 2, "fraction": 1.0, "overwrite": False},
}

train_dir = "/content/datasetsplit/train"

for folder in os.listdir(train_dir):
    folder_path = os.path.join(train_dir, folder)
    if not os.path.isdir(folder_path):
        continue

    config = augmentation_config.get(folder)
    if config is None:
        print(f"Non config for {folder}, jumping to next one")
        continue

    all_images = sorted(glob.glob(os.path.join(folder_path, "*.jpg")))

    selected_count = int(len(all_images) * config["fraction"])
    selected_images = random.sample(all_images, selected_count)

    print(f"\nClass: {folder}")
    print(f"Total: {len(all_images)} | Choosen: {selected_count} | Overwrite: {config['overwrite']}")

    for idx, img_path in enumerate(tqdm(selected_images, desc=f"{folder}")):
        img = Image.open(img_path)
        sp_img = add_salt_and_pepper(img, amount=0.02)

        if config["overwrite"]:
            sp_img.save(img_path)
        else:
            base = os.path.splitext(os.path.basename(img_path))[0]
            for i in range(config["num_augment"]):
                save_name = f"{base}_sp{i+1}.jpg"
                save_path = os.path.join(folder_path, save_name)
                sp_img_aug = add_salt_and_pepper(img, amount=0.02)
                sp_img_aug.save(save_path)

print("Salt and pepper augmentation is complete")

base_path = "/content/datasetsplit"
splits = ["train", "val", "test"]

for split in splits:
    print(f"\n{split.upper()}:")

    split_dir = os.path.join(base_path, split)
    class_counts = defaultdict(int)
    total = 0

    for class_folder in os.listdir(split_dir):
        class_path = os.path.join(split_dir, class_folder)
        if not os.path.isdir(class_path):
            continue
        images = [f for f in os.listdir(class_path) if f.lower().endswith((".jpg"))]
        class_counts[class_folder] += len(images)
        total += len(images)

    for cls, count in sorted(class_counts.items()):
        print(f" - {cls}: {count} images")
    print(f"Total in {split}: {total} images")

class_folder = "/content/datasetsplit/train/0_Mild_Dementia"

original_images = [f for f in os.listdir(class_folder)
                   if f.lower().endswith((".jpg")) and "_sp" not in f]

sample_file = random.choice(original_images)
img_path = os.path.join(class_folder, sample_file)

original_img = Image.open(img_path)
rgb_img = original_img.convert("RGB").resize((224, 224))
sp_img = add_salt_and_pepper(rgb_img, amount=0.02)

plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.imshow(original_img, cmap='gray')
plt.title("Original (Grayscale)")
plt.axis("off")

plt.subplot(1, 3, 2)
plt.imshow(rgb_img)
plt.title("After RGB + Resize(224x224)")
plt.axis("off")

plt.subplot(1, 3, 3)
plt.imshow(sp_img)
plt.title("Salt & Pepper")
plt.axis("off")

plt.tight_layout()
plt.show()

train_dir = "/content/datasetsplit/train"
class_counts = {}

for class_name in os.listdir(train_dir):
    class_path = os.path.join(train_dir, class_name)
    if not os.path.isdir(class_path):
        continue
    label = int(class_name.split("_")[0])
    count = len([f for f in os.listdir(class_path) if f.lower().endswith((".jpg"))])
    class_counts[label] = count

total_images = sum(class_counts.values())

class_weights = {}
for label, count in class_counts.items():
    class_weights[label] = total_images / (len(class_counts) * count)

sorted_weights = [class_weights[i] for i in sorted(class_weights.keys())]
class_weights_tensor = torch.FloatTensor(sorted_weights)

print("Class weights:")
for i, w in enumerate(sorted_weights):
    print(f"Class {i}: {w:.4f}")

def extract_patient_ids(folder_path):
    ids = set()
    for root, _, files in os.walk(folder_path):
        for f in files:
            match = re.match(r"(OAS1_\d+)_", f)
            if match:
                ids.add(match.group(1))
    return ids

train_ids = extract_patient_ids("/content/dataset/train")
val_ids = extract_patient_ids("/content/dataset/val")
test_ids = extract_patient_ids("/content/dataset/test")

overlap_val = train_ids.intersection(val_ids)
overlap_test = train_ids.intersection(test_ids)
overlap_val_test = val_ids.intersection(test_ids)

print(f"Train - Val overlap: {len(overlap_val)}")
print(f"Train - Test overlap: {len(overlap_test)}")
print(f"Val - Test overlap: {len(overlap_val_test)}")

imagenet_mean = [0.485, 0.456, 0.406]
imagenet_std  = [0.229, 0.224, 0.225]

train_transforms = transforms.Compose([
    transforms.RandomRotation(degrees=10),
    transforms.RandomAffine(
        degrees=0,
        translate=(0.05, 0.05),
        scale=(0.95, 1.05),
        fill=0
    ),
    transforms.ColorJitter(brightness=(0.9, 1.1)),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)
])

val_test_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)
])

class BrainMRIDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.image_paths = []
        self.labels = []

        for folder in os.listdir(root_dir):
            folder_path = os.path.join(root_dir, folder)
            if not os.path.isdir(folder_path):
                continue
            label = int(folder.split("_")[0])
            for fname in os.listdir(folder_path):
                if fname.lower().endswith((".jpg")):
                    self.image_paths.append(os.path.join(folder_path, fname))
                    self.labels.append(label)

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        label = self.labels[idx]

        image = Image.open(img_path).convert("RGB")
        if self.transform:
            image = self.transform(image)

        return image, label

train_path = "/content/datasetsplit/train"
val_path   = "/content/datasetsplit/val"
test_path  = "/content/datasetsplit/test"

train_dataset = BrainMRIDataset(train_path, transform=train_transforms)
val_dataset   = BrainMRIDataset(val_path, transform=val_test_transforms)
test_dataset  = BrainMRIDataset(test_path, transform=val_test_transforms)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)
val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)
test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)

print("DataLoaders is ready")
print(f"Train: {len(train_dataset)} images | Val: {len(val_dataset)} images| Test: {len(test_dataset)} images")

model = timm.create_model("vit_base_patch16_224", pretrained=True, drop_rate=0.1)

model.head = nn.Linear(model.head.in_features, 4)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

criterion = nn.CrossEntropyLoss(weight=class_weights_tensor.to(device))

optimizer = optim.Adam(model.parameters(), lr=0.0001)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.5,
    patience=4,
    min_lr=0.000001,
    verbose=True
)

print(model)
print("Model and trainingset is ready")

def train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=20, patience=7):
    best_model_wts = copy.deepcopy(model.state_dict())
    best_val_loss = float("inf")
    best_epoch = 0
    trigger_times = 0

    train_losses, val_losses = [], []
    train_accs, val_accs = [], []

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}")
        epoch_start = time.time()

        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
                dataloader = train_loader
            else:
                model.eval()
                dataloader = val_loader

            running_loss = 0.0
            running_corrects = 0

            loop = tqdm(dataloader, desc=f"{phase.capitalize()} Epoch {epoch+1}/{num_epochs}", leave=False)

            for inputs, labels in loop:
                inputs = inputs.to(device)
                labels = labels.to(device)

                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

                loop.set_postfix({
                    "Loss": f"{loss.item():.4f}",
                    "Batch Acc": f"{(torch.sum(preds == labels.data).item() / inputs.size(0)) * 100:.1f}%"
                })

            epoch_loss = running_loss / len(dataloader.dataset)
            epoch_acc = running_corrects.double() / len(dataloader.dataset)

            print(f"{phase.capitalize()} | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.4f}")

            if phase == 'train':
                train_losses.append(epoch_loss)
                train_accs.append(epoch_acc)
            else:
                val_losses.append(epoch_loss)
                val_accs.append(epoch_acc)
                scheduler.step(epoch_loss)

                if epoch_loss < best_val_loss:
                    best_val_loss = epoch_loss
                    best_model_wts = copy.deepcopy(model.state_dict())
                    best_epoch = epoch
                    trigger_times = 0
                else:
                    trigger_times += 1
                    if trigger_times >= patience:
                        print(f"\nEarly stopping triggered at epoch {epoch+1}")
                        model.load_state_dict(best_model_wts)
                        return model, train_losses, val_losses, train_accs, val_accs

        epoch_time = time.time() - epoch_start
        print(f"Epoch done in {epoch_time:.2f} sek")

    print(f"\nBest model at epoch {best_epoch+1} with val loss {best_val_loss:.4f}")
    model.load_state_dict(best_model_wts)
    torch.save(model.state_dict(), "best_model.pth")
    return model, train_losses, val_losses, train_accs, val_accs

for inputs, _ in train_loader:
    print(inputs.shape)
    break

model, train_losses, val_losses, train_accs, val_accs = train_model(
    model,
    criterion,
    optimizer,
    scheduler,
    train_loader,
    val_loader,
    num_epochs=20,
    patience=7
)

epochs = range(1, len(train_losses) + 1)

plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(epochs, train_losses, label='Train Loss')
plt.plot(epochs, val_losses, label='Val Loss')
plt.xlabel("Epoch", fontsize=12)
plt.ylabel("Loss", fontsize=12)
plt.title("Training vs Validation Loss", fontsize=14)
plt.legend()
plt.grid(False)

plt.subplot(1, 2, 2)
plt.plot(epochs, [acc.item() for acc in train_accs], label='Train Accuracy')
plt.plot(epochs, [acc.item() for acc in val_accs], label='Val Accuracy')
plt.xlabel("Epoch", fontsize=12)
plt.ylabel("Accuracy", fontsize=12)
plt.title("Training vs Validation Accuracy", fontsize=14)
plt.ylim(0, 1)
plt.legend()
plt.grid(False)

plt.tight_layout()
plt.show()

model.eval()

all_preds = []
all_labels = []

with torch.no_grad():
    for inputs, labels in test_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

print("Classification Report Novel Approach:\n")
print(classification_report(
    all_labels,
    all_preds,
    target_names=["Non Demented", "Very mild Dementia", "Mild Dementia", "Moderate Dementia"],
    digits=4
))

cm = confusion_matrix(all_labels, all_preds)
cm_percent = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(8, 6))
sns.heatmap(cm_percent, annot=True, fmt=".4f", cmap="Blues",
            xticklabels=["Non Demented", "Very mild Dementia", "Mild Dementia", "Moderate Dementia"],
            yticklabels=["Non Demented", "Very mild Dementia", "Mild Dementia", "Moderate Dementia"])

plt.xticks(rotation=45)
plt.xlabel("Predicted label")
plt.ylabel("True label")
plt.title("Confusion Matrix Novel Approach")
plt.show()

"""Next section is XAI"""

!pip install lime
!pip install lime scikit-image

from lime import lime_image

def predict_fn(images):
    model.eval()
    batch = torch.stack([
        val_test_transforms(Image.fromarray(img)).to(device)
        for img in images
    ])
    with torch.no_grad():
        outputs = model(batch)
        probs = torch.nn.functional.softmax(outputs, dim=1)
    return probs.cpu().numpy()

bilde_path = "/content/datasetsplit/test/1_Moderate_Dementia/OAS1_0308_MR1_mpr-1_142.jpg"
img = Image.open(bilde_path).convert("RGB")
img_np = np.array(img.resize((224, 224)))

explainer = lime_image.LimeImageExplainer()
explanation = explainer.explain_instance(
    image=img_np,
    classifier_fn=predict_fn,
    top_labels=4,
    hide_color=0,
    num_samples=1000
)

pred_label = explanation.top_labels[0]
temp, mask = explanation.get_image_and_mask(
    label=pred_label,
    positive_only=True,
    hide_rest=True,
    num_features=8,
    min_weight=0.1
)

plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.imshow(img_np)
plt.title("Original Image")
plt.axis("off")

plt.subplot(1, 2, 2)
plt.imshow(mark_boundaries(temp, mask))
plt.title(f"LIME Explanation (Pred: {pred_label})")
plt.axis("off")

plt.tight_layout()
plt.show()

image_paths = [
    "/content/datasetsplit/test/0_Mild_Dementia/OAS1_0028_MR1_mpr-1_103.jpg",
    "/content/datasetsplit/test/1_Moderate_Dementia/OAS1_0308_MR1_mpr-1_133.jpg",
    "/content/datasetsplit/test/2_Non_Demented/OAS1_0001_MR1_mpr-1_122.jpg",
    "/content/datasetsplit/test/3_Very_mild_Dementia/OAS1_0016_MR1_mpr-3_121.jpg"
]

originals = []
explanations = []
titles = []

for path in image_paths:
    img = Image.open(path).convert("RGB")
    img_resized = img.resize((224, 224))
    transformed_img = transform(img_resized)

    input_tensor = transformed_img.unsqueeze(0).to("cuda")
    with torch.no_grad():
        output = model(input_tensor)
        probs = torch.nn.functional.softmax(output, dim=1)
        pred_class = torch.argmax(probs, dim=1).item()
        confidence = probs[0][pred_class].item()

    explanation = explainer.explain_instance(
        image=np.array(img_resized),
        classifier_fn=predict_fn,
        top_labels=4,
        hide_color=0,
        num_samples=1000
    )

    temp, mask = explanation.get_image_and_mask(
        label=pred_class,
        positive_only=True,
        hide_rest=True,
        num_features=8,
        min_weight=0.1
    )

    originals.append(img_resized)
    explanations.append((temp, mask))
    titles.append(f"{class_names[pred_class]} ({confidence*100:.2f}%)")

fig, axes = plt.subplots(4, 2, figsize=(10, 16))

for i in range(4):
    axes[i, 0].imshow(originals[i])
    axes[i, 0].set_title(f"Original Image {i+1}")
    axes[i, 0].axis("off")

    temp, mask = explanations[i]
    axes[i, 1].imshow(mark_boundaries(temp, mask))
    axes[i, 1].set_title(f"LIME: {titles[i]}")
    axes[i, 1].axis("off")

plt.tight_layout()
plt.show()